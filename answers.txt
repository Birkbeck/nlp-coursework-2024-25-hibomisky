Answers to the text questions go here.
Q1 (d): When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of 4
text difficulty? Give two conditions. (Text answer, 200 words maximum).

A1 (d): The reason that its not a valid estimator is because its a formular that does not understand the meaning of the wo.rds.
It can fail if the text has a lot of technical jargon such a medical peer reviewed journals with lots of long medical words (neurooncological)
A clinican has no issues reading those words but the formula thinks its a diffuclt word due to how long it is

The second reason it doesn't work well is because it struggles with docuements that have complex concepts such as love poem or a philosophy book
Some book/poems etc might use short everyday words but the context and the emotional depth is complicated and hard to understand so the formular struggles.
The score measures word/sentene length (simple metrics) and not deep/abstract ideas.

F-K can fail for translated docs if sentence structures are different than english sentence structures


Q2 (f): my custom tokeniser makes the text cleaner by removing special charaters like numbers (covid19 becomes covid for example) using RegEx
It also makes all text lowercase (NHS becomes nhs for example)
Stopword removal also takes out common words like the/and/it etc
It uses WordNetLemmantizer which breaks words down to their root (like running turns to run)
this reduces noise and standardises words to their core roots

PERFORMANCE:
tokenizer improved SVM macro F1 score from 0.605 to 0.609 
tokenizer reduced SVM accruacy from 0.812 to 0.805
svm classifers had the highest accuracy with ngrams one having a score 0.817 compared to custom tokenizer accuracy (only 0.805) and baseline svm  (0.812)
custom svm had best class balance with highest macro F1 scores (0.609) compared to ngrams svm (only 0.600) and baseline svm  (0.605)

there is a class imbalance issue in the dataset for the lib dems (they only had 54 samples)
also there's poor minority class performance for lib dems (f1 scoere 0.16)
this cuases a high false positive for lib dems in custom svm (4.8% recall)
the model is not predictiing some classes at all so precison is not good because you can't divide by 0 predicted samples

To fix this class imbalance you could use SMOTE (synthetic minority oversampling) before training/after vectorization
it makes fake minority samples and balances existing data without losing infomation
it works very well with TF IDF, after vectorization but before training
it would create a better politically representative classifier and keep high accuracy
